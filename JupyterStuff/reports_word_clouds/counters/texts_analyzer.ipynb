{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_array(file_name: str):\n",
    "    try:\n",
    "        with open(file_name) as file:\n",
    "            lines = file.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_name, encoding=\"cp1251\") as file:\n",
    "            lines = file.readlines()\n",
    "    return [s.replace(\"\\n\", \"\") for s in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def filter_normalize(text):\n",
    "    return ' '.join(\n",
    "        list(map(lambda l: morph.parse(l)[0].normal_form, \n",
    "        re.sub(r\"[\\n\\r\\t]\", ' ', text).split(\" \")\n",
    "                )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = lines_to_array(\"./trash/pos.txt\")\n",
    "s_positive_words = lines_to_array(\"./trash/strong_pos.txt\")\n",
    "negative_words = lines_to_array(\"./trash/neg.txt\")\n",
    "s_negative_words = lines_to_array(\"./trash/strong_neg.txt\")\n",
    "words_by_sentiment = {\"positive\": list(map(lambda w: filter_normalize(w), positive_words)),\n",
    "                      \"strongly_positive\": list(map(lambda w: filter_normalize(w), s_positive_words)),\n",
    "                      \"negative\": list(map(lambda w: filter_normalize(w), negative_words)),\n",
    "                      \"strongly_negative\": list(map(lambda w: filter_normalize(w), s_negative_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = lines_to_array(\"./trash/positive.txt\")\n",
    "s_positive_words = lines_to_array(\"./trash/strongly_positive.txt\")\n",
    "negative_words = lines_to_array(\"./trash/negative.txt\")\n",
    "s_negative_words = lines_to_array(\"./trash/strongly_negative.txt\")\n",
    "words_by_sentiment = {\"positive\": list(map(lambda w: filter_normalize(w), positive_words)),\n",
    "                      \"strongly_positive\": list(map(lambda w: filter_normalize(w), s_positive_words)),\n",
    "                      \"negative\": list(map(lambda w: filter_normalize(w), negative_words)),\n",
    "                      \"strongly_negative\": list(map(lambda w: filter_normalize(w), s_negative_words))}\n",
    "sentiment_duplicates = get_sentiment_duplicates(words_by_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_words = lines_to_array(\"./trash/words_overall.txt\")\n",
    "recent_words = list(map(lambda w: filter_normalize(w), recent_words))\n",
    "recent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_text, normalized_reports = load_reports(\"./reports\")\n",
    "sentiment_results = {}\n",
    "counting_results = {}\n",
    "words_charact = {}\n",
    "for rep, text in normalized_reports.items():\n",
    "    sentiment_results[rep] = sentiment_analysis(text, words_by_sentiment, sentiment_duplicates)\n",
    "    counting_results[rep] = count_words(text, overall_words)\n",
    "    text_len, avg_w_len = words_characteristics(text)\n",
    "    words_charact[rep] = {\"Длина в тексте\": text_len,\n",
    "                          \"Средняя длина слова\": avg_w_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reports(directory):\n",
    "    normalized_reports = {}\n",
    "    reports_text = {}\n",
    "    print(directory)\n",
    "    for file in os.listdir(directory):\n",
    "        full_path = f\"{directory}/{file}\"\n",
    "        if os.path.isfile(full_path) and full_path.endswith(\".txt\"):\n",
    "            with open(full_path) as current_text:\n",
    "                print(f\"Processing of {file} has been started\")\n",
    "                reports_text[file] = re.sub(r\"[^а-яА-Я]\", ' ', ''.join(current_text.readlines())).lower()\n",
    "                normalized_reports[file] = filter_normailize(reports_text[file])\n",
    "    return normalized_reports, reports_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_words = {\"годовой отчет\", \"год\", \"также\", \"быть\", \"при\", \"который\", \"тот\", \"или\"}\n",
    "\n",
    "def words_characteristics(text):\n",
    "    overall_len = 0\n",
    "    for sw in garbage_words:\n",
    "        rep = text.replace(sw, '')\n",
    "    splitted_filtered = list(filter(lambda s: len(s) > 2, text.split(\" \")))\n",
    "    text_length = len(splitted_filtered)\n",
    "    for wrd in splitted_filtered:\n",
    "        overall_len += len(wrd)\n",
    "    return text_length, int(overall_len / (1 if text_length == 0 else text_length))\n",
    "\n",
    "def sentiment_analysis(text, words_by_sentiment, duplicates):\n",
    "    sent_counts = {sentiment: 0 for sentiment in words_by_sentiment.keys()}\n",
    "    for sent, words in words_by_sentiment.items():\n",
    "        for w in words:\n",
    "            sent_counts[sent] += text.count(w)\n",
    "            if w in duplicates.keys():\n",
    "                sent_counts[sent] -= duplicates[w]\n",
    "    return sent_counts\n",
    "\n",
    "def get_sentiment_duplicates(sentiment_words):\n",
    "    strongly_pos = set(sentiment_words['strongly_positive'])\n",
    "    pos = set(sentiment_words['positive'])\n",
    "    all_negative = ','.join(sentiment_words['strongly_negative'] + sentiment_words['negative'])\n",
    "    occured_words = {}\n",
    "    for w in strongly_pos.union(pos):\n",
    "        occurences = len(re.findall(fr\"не( )?{w}\", all_negative))\n",
    "        if occurences != 0:\n",
    "            if len(w.split(\" \")) == 2 and len(w.split(\" \")[0]) == 2:\n",
    "                occured_words[f\"не {w}\"] = occurences\n",
    "            else:\n",
    "                occured_words[f\"не{w}\"] = occurences\n",
    "    return occured_words\n",
    "            \n",
    "def count_words(text: str, words_to_count: list):\n",
    "    counted = {word: 0 for word in words_to_count}\n",
    "    for w in counted.keys():\n",
    "        counted[w] += len(re.findall(fr\"\\b{w}\\b\", text))\n",
    "    return counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(companies_words_count2).to_csv(\"counted_no_duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(companies_words_count).to_csv(\"words_counted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkeys = list(overall_words)\n",
    "recent_words.sort(key=lambda l: len(l))\n",
    "matched_words = defaultdict(set)\n",
    "for w in wkeys:\n",
    "    for wi in range(wkeys.index(w) + 1, len(wkeys)):\n",
    "        if w in wkeys[wi].strip() and w != wkeys[wi]:\n",
    "            matched_words[w].add(wkeys[wi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_words_count = copy.deepcopy(counting_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words_counts in companies_words_count.values():\n",
    "    for word, matches in sorted(matched_words.items(), key = lambda k: len(k[0]), reverse = True):\n",
    "        for w in matches:\n",
    "            words_counts[word] -= words_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"trash/eco_words.txt\") as eco:\n",
    "    eco_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in eco.readlines()}\n",
    "\n",
    "with open(\"trash/soc_words.txt\") as soc:\n",
    "    soc_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in soc.readlines()}\n",
    "    \n",
    "with open(\"trash/corp.txt\") as corp:\n",
    "    corp_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in corp.readlines()}\n",
    "\n",
    "overall_words = eco_words.union(soc_words).union(corp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"trash/ecologyv2.txt\", encoding=\"cp1251\") as eco:\n",
    "    eco_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in eco.readlines()}\n",
    "\n",
    "with open(\"trash/socialv2.txt\", encoding=\"cp1251\") as soc:\n",
    "    soc_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in soc.readlines()}\n",
    "    \n",
    "with open(\"trash/corp_govv2.txt\", encoding=\"cp1251\") as corp:\n",
    "    corp_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in corp.readlines()}\n",
    "    \n",
    "overall_words = eco_words.union(soc_words).union(corp_words)\n",
    "overall_words.remove(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trash/ecologyv3.txt\", encoding=\"cp1251\") as eco:\n",
    "    eco_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in eco.readlines()}\n",
    "\n",
    "with open(\"trash/socialv3.txt\", encoding=\"cp1251\") as soc:\n",
    "    soc_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in soc.readlines()}\n",
    "    \n",
    "with open(\"trash/corp_govv3.txt\", encoding=\"cp1251\") as corp:\n",
    "    corp_words = {filter_normailize(s.replace(\"\\n\", \"\")) for s in corp.readlines()}\n",
    "    \n",
    "overall_words = eco_words.union(soc_words).union(corp_words)\n",
    "#overall_words.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_comp_words = {}\n",
    "soc_comp_words = {}\n",
    "corp_comp_words = {}\n",
    "\n",
    "for k, words_counts in companies_words_count.items():\n",
    "    eco_comp_words[k] = {eco_key: companies_words_count[k].get(eco_key, 0) for eco_key in eco_words}\n",
    "    soc_comp_words[k] = {eco_key: companies_words_count[k].get(eco_key, 0) for eco_key in soc_words}\n",
    "    corp_comp_words[k] = {eco_key: companies_words_count[k].get(eco_key, 0) for eco_key in corp_words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(eco_comp_words).to_csv(\"eco_comp_words.csv\", encoding=\"utf-8\")\n",
    "pd.DataFrame(soc_comp_words).to_csv(\"soc_comp_words.csv\", encoding=\"utf-8\")\n",
    "pd.DataFrame(corp_comp_words).to_csv(\"corp_comp_words.csv\", encoding=\"utf-8\")\n",
    "pd.DataFrame(companies_reports).to_csv(\"companies_reports.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_comp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentiment_results).to_csv(\"sentiment_results.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(words_charact).to_csv(\"words_char.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_comp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
